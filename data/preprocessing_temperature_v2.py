# -*- coding: utf-8 -*-
"""preprocessing_temperature_v2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1bA5YPoZpCIhwv337pxRmd60JgwKJ9XMp
"""

!pip install netCDF4
!pip install pyproj

# install basemap on google colab
!apt-get install libgeos-3.5.0
!apt-get install libgeos-dev
!pip install https://github.com/matplotlib/basemap/archive/master.zip

# import packages
get_ipython().run_line_magic('matplotlib', 'inline')
from scipy.io import netcdf
from netCDF4 import Dataset as NetCDFFile 
import pyproj
import numpy as np
import matplotlib.pyplot as plt
from mpl_toolkits.basemap import Basemap
from datetime import date
from dateutil.relativedelta import relativedelta
import ftplib
import os

from google.colab import drive
drive.mount('/content/drive')

def view_data(filename):
  # Opens up the file to view its content, such as variables and dimensions.
  # Assumes netcdf file format.
    data = NetCDFFile(filename)
    print(data)
    print(data.variables['time'])

def FTPimprort(FILE_NAME):
  path = '/Datasets/cpc_global_temp/' # path is the location of the file in the ftp server
  FILE_NAME = 't%s.%d.nc' % (data_type, year) # filename is the name + extension of the file 

  # connect to FTP server and download file
  # https://psl.noaa.gov/thredds/catalog/Datasets/cpc_global_temp/catalog.html
  # ftp login is anonymous
  # ftp.cwd will change the current working directory to where the file is located in order to download it
  # retrbinary will get the file from the server and store in your local machine using the same name it had on the server
  ftp = ftplib.FTP("ftp2.psl.noaa.gov") 
  ftp.login() 
  ftp.cwd(path)
  ftp.retrbinary("RETR " + FILE_NAME ,open(FILE_NAME, 'wb').write)
  ftp.quit()

  # load NetCDF file
  nc_data = NetCDFFile(FILE_NAME)

  # remove downloaded file
  os.remove(FILE_NAME)

  return nc_data

def get_area_coords(coordinate_list, lims):
  idx = []
  for lim in lims:
    minimum = float("inf")
    for i in range(len(coordinate_list)):
      if abs(lim - coordinate_list[i]) < minimum:
          final_value = i
          minimum = abs(lim - coordinate_list[i])
    idx.append(final_value)
  area_coords = coordinate_list[idx[0]: idx[1]]
  return area_coords, idx

def extract_temp_data(data, lat_lims, lon_lims, data_type):
  values = data.variables['t'+ data_type][:]  # tmin or tmax
  time = data.variables['time'][:]
  lat = data.variables['lat'][:]
  lon = data.variables['lon'][:]
  area_lat, idx_lat = get_area_coords(lat, lat_lims[::-1]) # reverse latitide limits
  area_lon, idx_lon = get_area_coords(lon, lon_lims)
  # print(idx_lat, idx_lon)
  # print(lat[idx_lat[0]:idx_lat[1]])
  # print(lon[idx_lon[0]:idx_lon[1]])
  area_values = values[:, idx_lat[0]:idx_lat[1], idx_lon[0]:idx_lon[1]]
  dict_temp = {'values': values,
          'area_lat': area_lat,
          'area_lon': area_lon,
          'time': time,
          'area_values': area_values}
  return dict_temp

def average_temp_data(area_value):
  # Averages daily temperatures November to March.
  area_average = np.empty((0,22, 22))
  for i in range(1,6,1):
    area = np.mean(area_value[(305*i):(305*i)+150,:,:],axis=0)
    area_average = np.concatenate((area_average,np.reshape(area,(1,22,22))))

  # check shape
  # print(area_average.shape)
  # print(area_value.shape)
  return area_average

# input range of years
from_year = 2015
to_year = 2019
types = ["max", "min"]

# look at data variables
# view_data(path)

# input lat and lon over California
lat_lims = [31, 42] # latitude range over California
lon_lims = [235, 246] # longitude range over California

# for loop over the years
for year in range(from_year, to_year+1, 1): # to include to_year
    for data_type in types:
        FILE_NAME = 't%s.%d.nc' % (data_type, year) # filename + extension of the file
        # path = 'drive/Shareddrives/CS230 Project/preprocessing_temperature/' + FILE_NAME

        # load NetCDF file
        if data_type == "max":
          data = FTPimprort(FILE_NAME)
          dict_tmax[year] = extract_temp_data(data, lat_lims, lon_lims, data_type)
          print(FILE_NAME)
        else:
          data = FTPimprort(FILE_NAME)
          dict_tmin[year] = extract_temp_data(data, lat_lims, lon_lims, data_type)

# combine data from all years
tmin_all = np.concatenate((dict_tmin[2015]['area_values'],dict_tmin[2016]['area_values'],dict_tmin[2017]['area_values'],dict_tmin[2018]['area_values'],dict_tmin[2019]['area_values']))
tmax_all = np.concatenate((dict_tmax[2015]['area_values'],dict_tmax[2016]['area_values'],dict_tmax[2017]['area_values'],dict_tmax[2018]['area_values'],dict_tmax[2019]['area_values']))

# create average values
tmin_avg = average_temp_data(tmin_all)
tmax_avg = average_temp_data(tmax_all)

def plot_map(variable, lon, lat, title):
  # plot variable on map over California
  # specify map lat/lon limits
  lon_min = 235
  lat_min = 31
  lon_max = 245.5
  lat_max = 42.5

  map = Basemap(projection='merc',llcrnrlon=lon_min,llcrnrlat=lat_min,urcrnrlon=lon_max,urcrnrlat=lat_max,resolution='l') # projection, lat/lon extents and resolution of polygons to draw
  lons, lats = np.meshgrid(lon, lat)
  x, y = map(lons, lats)

  # add states and border masks
  plt.figure(figsize=(15,10))
  map.drawcoastlines()
  map.drawstates()
  map.drawcountries()

  # add lat/lon masks
  parallels = np.arange(lat_min,lat_max,5.) # make latitude lines ever 5 degrees from 30N-50N
  meridians = np.arange(lon_min,lon_max,5.) # make longitude lines every 5 degrees from 95W to 70W
  map.drawparallels(parallels,labels=[1,0,0,0],fontsize=10)
  map.drawmeridians(meridians,labels=[0,0,0,1],fontsize=10)

  # plot data
  z = map.contourf(x, y, variable)
  cb = map.colorbar(z, "right", size="5%", pad="2%")
  plt.title(title)
  cb.set_label('Value')
  plt.show()

# plot_map(temp_area_values[14,:,:], tmin_area_lon, tmin_area_lat, 'TMIN')
